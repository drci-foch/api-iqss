"""
Module de traitement et d'analyse des donnÃ©es
Version R v7 - DÃ©cembre 2025
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import numpy as np
import unicodedata
from database import get_sejours_data, get_documents_data
from config import settings


def normalize_text(text):
    """Normalise le texte: majuscules, sans accents"""
    if pd.isna(text):
        return None
    text = str(text).upper().strip()
    text = "".join(
        c for c in unicodedata.normalize("NFD", text) if unicodedata.category(c) != "Mn"
    )
    return text


def load_matrice_specialite(
    matrice_path: str = None,
) -> pd.DataFrame:
    """
    Charge et prÃ©pare la matrice de spÃ©cialitÃ© v7

    Args:
        matrice_path: Chemin vers le fichier Excel de mapping

    Returns:
        DataFrame avec les mappings UF/doc_key -> spÃ©cialitÃ©
    """
    if matrice_path is None:
        matrice_path = settings.MATRICE_PATH

    try:
        # Lire le fichier Excel (v7 utilise .xlsx au lieu de .csv)
        matrice = pd.read_excel(matrice_path, dtype={"sej_uf": str})
        matrice["doc_key_norm"] = matrice["doc_key"].apply(normalize_text)
        # Supprimer les doublons (garder la premiÃ¨re occurrence)
        matrice = matrice.drop_duplicates(
            subset=["sej_uf", "doc_key_norm"], keep="first"
        )
        print(f"   âœ… Matrice chargÃ©e : {len(matrice)} mappings UF/spÃ©cialitÃ©")
        return matrice
    except FileNotFoundError:
        print(f"   âš ï¸ Fichier matrice non trouvÃ© : {matrice_path}")
        print(f"   âš ï¸ Tentative avec ancien format CSV...")
        # Fallback vers CSV si Excel non trouvÃ©
        csv_path = matrice_path.replace(".xlsx", ".csv")
        try:
            matrice = pd.read_csv(csv_path, dtype={"sej_uf": str})
            matrice["doc_key_norm"] = matrice["doc_key"].apply(normalize_text)
            matrice = matrice.drop_duplicates(
                subset=["sej_uf", "doc_key_norm"], keep="first"
            )
            print(f"   âœ… Matrice CSV chargÃ©e : {len(matrice)} mappings")
            return matrice
        except Exception as e:
            raise FileNotFoundError(
                f"Impossible de charger la matrice (Excel ou CSV) : {e}"
            )
    except Exception as e:
        raise Exception(f"Erreur lors du chargement de la matrice : {e}")


def create_doc_key(libelle: str) -> str:
    """
    CrÃ©er une clÃ© de document Ã  partir du libellÃ©
    Simplifie et normalise le libellÃ© du document

    Args:
        libelle: LibellÃ© du document depuis EASILY

    Returns:
        ClÃ© simplifiÃ©e du document
    """
    if pd.isna(libelle):
        return ""

    # Normaliser
    key = str(libelle).lower().strip()

    # Supprimer les patterns inutiles (mÃªme logique que le code R)
    patterns_to_remove = [
        "cr lettre de liaison",
        "lettre de liaison",
        "cr",
        "foch",
        "hdj",
        "cs",
        "\\.",
        "ll",
    ]

    for pattern in patterns_to_remove:
        key = key.replace(pattern, "")

    key = key.strip()
    return key


def merge_sejours_documents(
    sejours: pd.DataFrame, documents: pd.DataFrame
) -> pd.DataFrame:
    """
    Fusionne les donnÃ©es de sÃ©jours et documents selon la mÃ©thodologie IQL R v7

    Nouveaux critÃ¨res de rattachement (v7) :
    1. sdt_docven : NumÃ©ro de venue correspond
    2. sdt_docval : Doc validÃ© aprÃ¨s entrÃ©e ET â‰¥ (sortie - 3j)
    3. sdt_smere : Fiche mÃ¨re crÃ©Ã©e/modifiÃ©e AVANT la sortie
    4. sdt_doccre : Doc crÃ©Ã© aprÃ¨s entrÃ©e - 5j
    5. sdt_doccref : Doc crÃ©Ã© DURANT le sÃ©jour (critÃ¨re prÃ©fÃ©rentiel)
    6. sdt_emere : Fiche mÃ¨re crÃ©Ã©e/modifiÃ©e aprÃ¨s entrÃ©e - 5j

    CritÃ¨re minimal : (sdt_docval + sdt_smere + sdt_doccre) > 2

    Priorisation (ordre de tri) :
    1. PrÃ©sence de spÃ©cialitÃ© (sej_spe)
    2. Venue correspondante (sdt_docven)
    3. Fiche mÃ¨re valide (sdt_emere)
    4. CritÃ¨res minimaux OK (sdt_status)
    5. Doc crÃ©Ã© durant sÃ©jour (sdt_doccref)
    6. DÃ©lai de validation (del_sorval)

    Args:
        sejours: DataFrame des sÃ©jours (GAM)
        documents: DataFrame des documents (EASILY)

    Returns:
        DataFrame avec UN sÃ©jour par ligne et son document optimal
    """
    sejours = sejours.copy()
    documents = documents.copy()

    print("\nðŸ”— Fusion sÃ©jours Ã— documents (mÃ©thodologie R v7)...")

    # PrÃ©parer les clÃ©s de jointure
    sejours["pat_ipp"] = sejours["pat_ipp"].astype(str)
    documents["pat_ipp"] = documents["pat_ipp"].astype(str)

    # CrÃ©er les clÃ©s de documents si nÃ©cessaire
    if "doc_key" not in documents.columns:
        documents["doc_key"] = documents["doc_libelle"].apply(create_doc_key)

    print(f"   ðŸ“Š {len(sejours)} sÃ©jours Ã— {len(documents)} documents")

    # Fusionner sur l'IPP
    data = sejours.merge(documents, on="pat_ipp", how="left", suffixes=("", "_doc"))

    print(f"   âœ… Jointure IPP : {len(data)} lignes")

    # Convertir les dates
    data["sej_sor"] = pd.to_datetime(data["sej_sor"])
    data["sej_ent"] = pd.to_datetime(data["sej_ent"])
    data["doc_val"] = pd.to_datetime(data["doc_val"])
    data["doc_cre"] = pd.to_datetime(data["doc_cre"])

    if "doc_creamere" in data.columns:
        data["doc_creamere"] = pd.to_datetime(data["doc_creamere"])
    if "doc_modmere" in data.columns:
        data["doc_modmere"] = pd.to_datetime(data["doc_modmere"])

    print("\nðŸ” Application des critÃ¨res de rattachement R v7...")

    # ========================================
    # NOUVEAUX CRITÃˆRES BOOLÃ‰ENS (R v7)
    # ========================================

    # 1. sdt_docven : Le numÃ©ro de venue du doc est-il celui du sÃ©jour ?
    if "doc_venue" in data.columns:
        data["sdt_docven"] = data["sej_id"] == data["doc_venue"].astype(str)
        print(f"   âœ… sdt_docven : {data['sdt_docven'].sum()} correspondances venue")
    else:
        data["sdt_docven"] = False
        print(f"   âš ï¸ sdt_docven : colonne doc_venue absente, critÃ¨re dÃ©sactivÃ©")

    # 2. sdt_docval : Doc validÃ© aprÃ¨s entrÃ©e ET â‰¥ (sortie - 3 jours)
    data["sdt_docval"] = (data["doc_val"] >= data["sej_ent"]) & (
        data["doc_val"] >= (data["sej_sor"] - pd.Timedelta(days=3))
    )
    print(f"   âœ… sdt_docval : {data['sdt_docval'].sum()} docs dans fenÃªtre temporelle")

    # 3. sdt_smere : Fiche mÃ¨re crÃ©Ã©e OU modifiÃ©e AVANT la sortie
    if "doc_creamere" in data.columns and "doc_modmere" in data.columns:
        data["sdt_smere"] = (
            data["doc_creamere"].isna()
            | (data["doc_creamere"] <= data["sej_sor"])
            | (data["doc_modmere"] <= data["sej_sor"])
        )
        print(f"   âœ… sdt_smere : {data['sdt_smere'].sum()} fiches mÃ¨res avant sortie")
    else:
        data["sdt_smere"] = True  # Par dÃ©faut si pas de fiche mÃ¨re
        print(f"   âš ï¸ sdt_smere : colonnes fiche mÃ¨re absentes, critÃ¨re dÃ©sactivÃ©")

    # 4. sdt_doccre : Doc crÃ©Ã© aprÃ¨s entrÃ©e - 5 jours
    data["sdt_doccre"] = data["doc_cre"] >= (data["sej_ent"] - pd.Timedelta(days=5))
    print(f"   âœ… sdt_doccre : {data['sdt_doccre'].sum()} docs crÃ©Ã©s aprÃ¨s entrÃ©e-5j")

    # 5. sdt_doccref : Doc crÃ©Ã© DURANT le sÃ©jour (critÃ¨re prÃ©fÃ©rentiel)
    data["sdt_doccref"] = (data["doc_cre"] >= data["sej_ent"]) & (
        data["doc_cre"] <= data["sej_sor"]
    )
    print(f"   âœ… sdt_doccref : {data['sdt_doccref'].sum()} docs crÃ©Ã©s durant sÃ©jour")

    # 6. sdt_emere : Fiche mÃ¨re crÃ©Ã©e/modifiÃ©e aprÃ¨s entrÃ©e - 5j
    if "doc_creamere" in data.columns and "doc_modmere" in data.columns:
        data["sdt_emere"] = (
            data["doc_creamere"].isna()
            | (data["doc_creamere"] >= (data["sej_ent"] - pd.Timedelta(days=5)))
            | (data["doc_modmere"] >= (data["sej_ent"] - pd.Timedelta(days=5)))
        )
        print(
            f"   âœ… sdt_emere : {data['sdt_emere'].sum()} fiches mÃ¨res aprÃ¨s entrÃ©e-5j"
        )
    else:
        data["sdt_emere"] = True
        print(f"   âš ï¸ sdt_emere : colonnes fiche mÃ¨re absentes, critÃ¨re dÃ©sactivÃ©")

    # ========================================
    # CRITÃˆRE MINIMAL DE RATTACHEMENT
    # ========================================
    # Il faut au moins 3 critÃ¨res vrais parmi : docval, smere, doccre
    data["sdt_status"] = (
        data["sdt_docval"].astype(int)
        + data["sdt_smere"].astype(int)
        + data["sdt_doccre"].astype(int)
    ) > 2

    print(
        f"   âœ… sdt_status : {data['sdt_status'].sum()} lignes avec critÃ¨res minimaux OK"
    )

    # Calculer del_sorval seulement si critÃ¨res minimaux OK
    data["del_sorval"] = np.where(
        data["sdt_status"], (data["doc_val"] - data["sej_sor"]).dt.days, np.nan
    )

    nb_with_delay = data["del_sorval"].notna().sum()
    print(f"   âœ… del_sorval calculÃ© pour {nb_with_delay} lignes")

    # ========================================
    # JOINTURE AVEC MATRICE DE SPÃ‰CIALITÃ‰
    # ========================================
    # (pour pouvoir trier par sej_spe)

    print("\nðŸ¥ Jointure avec matrice de spÃ©cialitÃ©...")

    try:
        matrice = load_matrice_specialite(settings.MATRICE_PATH)

        # CrÃ©er doc_key_norm si nÃ©cessaire
        if "doc_key_norm" not in data.columns:
            data["doc_key_norm"] = data["doc_key"].apply(normalize_text)

        # Jointure
        data = data.merge(
            matrice[["sej_uf", "doc_key_norm", "sej_spe"]],
            on=["sej_uf", "doc_key_norm"],
            how="left",
            suffixes=("", "_matrice"),
        )

        nb_with_spe = data["sej_spe"].notna().sum()
        print(
            f"   âœ… SpÃ©cialitÃ© trouvÃ©e pour {nb_with_spe} lignes ({nb_with_spe / len(data) * 100:.1f}%)"
        )

    except Exception as e:
        print(f"   âš ï¸ Impossible de charger la matrice : {e}")
        print(f"   âš ï¸ Le tri par spÃ©cialitÃ© sera dÃ©sactivÃ©")
        data["sej_spe"] = None

    # ========================================
    # PRIORISATION DES DOCUMENTS (R v7)
    # ========================================
    # Trier selon l'ordre de prÃ©fÃ©rence R

    print("\nðŸ“Š Priorisation des documents (tri multi-critÃ¨res)...")

    # CrÃ©er une colonne boolÃ©enne : True si sej_spe existe, False sinon
    data["has_spe"] = data["sej_spe"].notna()

    # Pour chaque sÃ©jour, trier les documents candidats
    data_sorted = data.sort_values(
        by=[
            "sej_id",
            "has_spe",  # 1. Prioriser ceux avec spÃ©cialitÃ©
            "sdt_docven",  # 2. Prioriser si venue correspond
            "sdt_emere",  # 3. Prioriser si fiche mÃ¨re aprÃ¨s entrÃ©e
            "sdt_status",  # 4. Prioriser si critÃ¨res minimaux OK
            "sdt_doccref",  # 5. Prioriser si doc crÃ©Ã© durant sÃ©jour
            "del_sorval",  # 6. Puis trier par dÃ©lai (croissant)
        ],
        ascending=[True, False, False, False, False, False, True],
        na_position="last",
    )

    # Garder le meilleur document pour chaque sÃ©jour
    data_best = data_sorted.groupby("sej_id", as_index=False).first()

    print(f"   âœ… Meilleur document sÃ©lectionnÃ© pour {len(data_best)} sÃ©jours")

    # ========================================
    # GESTION DES DOCUMENTS MULTI-SÃ‰JOURS
    # ========================================
    # Si un document est associÃ© Ã  plusieurs sÃ©jours, ne garder que le plus proche

    print("\nðŸ”„ Gestion des documents multi-sÃ©jours...")

    # Pour chaque doc_id, compter combien de sÃ©jours l'utilisent
    doc_counts = data_best[data_best["doc_id"].notna()].groupby("doc_id").size()
    multi_sejour_docs = doc_counts[doc_counts > 1].index

    if len(multi_sejour_docs) > 0:
        print(f"   âš ï¸ {len(multi_sejour_docs)} documents associÃ©s Ã  plusieurs sÃ©jours")

        # Pour ces documents, marquer comme "libre" seulement le sÃ©jour le plus proche
        for doc_id in multi_sejour_docs:
            mask = data_best["doc_id"] == doc_id
            doc_sejours = data_best[mask].copy()

            # Trier par del_sorval (le plus proche)
            doc_sejours_sorted = doc_sejours.sort_values("del_sorval")

            # Seul le premier garde le document
            closest_sej = doc_sejours_sorted.iloc[0]["sej_id"]

            # Mettre del_sorval Ã  NaN pour les autres
            data_best.loc[mask & (data_best["sej_id"] != closest_sej), "del_sorval"] = (
                np.nan
            )

        print(f"   âœ… Documents multi-sÃ©jours traitÃ©s")
    else:
        print(f"   âœ… Aucun document multi-sÃ©jours")

    # ========================================
    # AJOUT DES SÃ‰JOURS SANS DOCUMENT
    # ========================================
    sejours_sans_doc = sejours[~sejours["sej_id"].isin(data_best["sej_id"])].copy()

    if len(sejours_sans_doc) > 0:
        print(f"   â„¹ï¸ {len(sejours_sans_doc)} sÃ©jours sans aucun document rattachÃ©")
        # Ajouter les colonnes manquantes avec NaN
        for col in data_best.columns:
            if col not in sejours_sans_doc.columns:
                sejours_sans_doc[col] = np.nan

        data_final = pd.concat([data_best, sejours_sans_doc], ignore_index=True)
    else:
        data_final = data_best

    # VÃ©rifications finales
    nb_sejours_initial = len(sejours)
    nb_sejours_final = len(data_final)

    print(
        f"\nâœ… Fusion terminÃ©e : {nb_sejours_initial} sÃ©jours â†’ {nb_sejours_final} lignes"
    )

    nb_avec_ll = data_final["doc_val"].notna().sum()
    print(
        f"ðŸ“Š Avec LL validÃ©e : {nb_avec_ll} ({nb_avec_ll / nb_sejours_final * 100:.1f}%)"
    )

    return data_final


def classify_sejours_iql(df: pd.DataFrame, matrice_path: str = None) -> pd.DataFrame:
    """
    Classifie les sÃ©jours selon la mÃ©thodologie IQL R v7

    Changements v7 :
    - Utilise del_val (â‰¥ 0) au lieu de del_sorval
    - del_val = max(0, del_sorval) si spÃ©cialitÃ© associÃ©e
    - Si la jointure avec la matrice a dÃ©jÃ  Ã©tÃ© faite dans merge_sejours_documents,
      on ne la refait pas

    RÃ¨gles de classification:
    - "0j" : LL validÃ©e au plus tard le jour de la sortie (del_val == 0)
    - "1j+" : LL validÃ©e aprÃ¨s la sortie (del_val > 0)
    - "sansLL" : Aucune LL validÃ©e OU pas de spÃ©cialitÃ© associÃ©e

    Args:
        df: DataFrame contenant les sÃ©jours et documents
        matrice_path: Chemin vers la matrice de spÃ©cialitÃ© (optionnel)

    Returns:
        DataFrame avec colonnes 'sej_spe_final' et 'sej_classe' ajoutÃ©es
    """
    df = df.copy()

    print("\nðŸ·ï¸ Classification des sÃ©jours (IQL R v7)...")

    # ========================================
    # VÃ‰RIFIER SI LA JOINTURE A DÃ‰JÃ€ Ã‰TÃ‰ FAITE
    # ========================================
    if "sej_spe" in df.columns and df["sej_spe"].notna().sum() > 0:
        print("   â„¹ï¸ SpÃ©cialitÃ©s dÃ©jÃ  jointes dans merge_sejours_documents()")
        df["sej_spe_final"] = df["sej_spe"]
    else:
        print("   â„¹ï¸ Jointure avec matrice de spÃ©cialitÃ© nÃ©cessaire")

        # Utiliser le chemin depuis settings si non fourni
        if matrice_path is None:
            matrice_path = settings.MATRICE_PATH

        # Charger la matrice de spÃ©cialitÃ©
        try:
            matrice = load_matrice_specialite(matrice_path)
        except Exception as e:
            print(f"âš ï¸ Erreur chargement matrice: {e}")
            # Fallback: utiliser doc_spe comme spÃ©cialitÃ©
            df["sej_spe_final"] = df.get("doc_spe")
            df["sej_classe"] = "sansLL"
            return df

        # PrÃ©parer les donnÃ©es pour le matching
        df["sej_uf"] = df["sej_uf"].astype(str)

        # CrÃ©er doc_key normalisÃ©e si nÃ©cessaire
        if "doc_key" not in df.columns:
            df["doc_key"] = df["doc_libelle"].apply(create_doc_key)

        df["doc_key_norm"] = df["doc_key"].apply(normalize_text)

        # Joindre avec la matrice de spÃ©cialitÃ©
        df = df.merge(
            matrice[["sej_uf", "doc_key_norm", "sej_spe"]],
            on=["sej_uf", "doc_key_norm"],
            how="left",
            suffixes=("_old", "_matrice"),
        )

        # DÃ©terminer la spÃ©cialitÃ© finale
        if "sej_spe_matrice" in df.columns:
            df["sej_spe_final"] = df["sej_spe_matrice"]
        elif "sej_spe" in df.columns:
            df["sej_spe_final"] = df["sej_spe"]
        else:
            df["sej_spe_final"] = None

    # ========================================
    # CALCULER del_val (R v7)
    # ========================================
    # del_val = max(0, del_sorval) si spÃ©cialitÃ© associÃ©e
    # Sinon NA

    print("\nðŸ“ Calcul de del_val (dÃ©lai rÃ©ajustÃ© â‰¥ 0)...")

    df["del_val"] = df.apply(
        lambda row: max(0, row["del_sorval"])
        if pd.notna(row["del_sorval"])
        and not np.isinf(row["del_sorval"])
        and pd.notna(row["sej_spe_final"])
        else np.nan,
        axis=1,
    )

    nb_with_delval = df["del_val"].notna().sum()
    print(f"   âœ… del_val calculÃ© pour {nb_with_delval} sÃ©jours")

    # ========================================
    # Classification selon del_val (pas del_sorval)
    # ========================================
    df["sej_classe"] = "sansLL"

    has_del_val = df["del_val"].notna()

    # Classification
    df.loc[has_del_val & (df["del_val"] == 0), "sej_classe"] = "0j"
    df.loc[has_del_val & (df["del_val"] > 0), "sej_classe"] = "1j+"

    print(f"\nðŸ“Š Classification finale :")
    for classe in ["0j", "1j+", "sansLL"]:
        count = (df["sej_classe"] == classe).sum()
        pct = count / len(df) * 100 if len(df) > 0 else 0
        print(f"   - {classe}: {count} ({pct:.1f}%)")

    return df


def calculate_validation_stats(df: pd.DataFrame, matrice_path: str = None) -> Dict:
    """
    Calcule les statistiques de validation selon la mÃ©thodologie IQL R v7

    Indicateurs HAS:
    1. % sÃ©jours avec LL retrouvÃ©e (classes "0j" + "1j+")
    2. % sÃ©jours avec LL datÃ©e du jour de la sortie (classe "0j")

    Args:
        df: DataFrame contenant les sÃ©jours et documents
        matrice_path: Chemin vers la matrice de spÃ©cialitÃ©

    Returns:
        Dictionnaire contenant les statistiques globales et par spÃ©cialitÃ©
    """

    # Utiliser le chemin depuis settings si non fourni
    if matrice_path is None:
        matrice_path = settings.MATRICE_PATH

    print(f"\nðŸ“Š Calcul des statistiques de VALIDATION...")

    # Classifier les sÃ©jours
    df = classify_sejours_iql(df, matrice_path)

    # Statistiques globales
    total_sejours_all = len(df)

    # =================TABLEAU GAELLE SUR VALIDATION==================
    nb_ll_validees_all = df["doc_val"].notna().sum()
    pct_ll_validees_all = df["doc_val"].notna().mean() * 100
    taux_validation_J0_over_sejours_all = float((df["sej_classe"] == "0j").mean() * 100)
    delai_validation_moyenne_all = df["del_sorval"].mean()

    print(f"\n   ðŸ“ˆ Statistiques globales :")
    print(f"      Total sÃ©jours : {total_sejours_all}")
    print(f"      LL validÃ©es : {nb_ll_validees_all} ({pct_ll_validees_all:.1f}%)")
    print(f"      ValidÃ©es Ã  J0 : {taux_validation_J0_over_sejours_all:.1f}%")
    print(f"      DÃ©lai moyen : {delai_validation_moyenne_all:.2f}j")

    # Statistiques par spÃ©cialitÃ©
    stats_par_spe = []

    for spe in df["sej_spe_final"].dropna().unique():
        df_spe = df[df["sej_spe_final"] == spe]
        total_sejours = len(df_spe)

        # =================TABLEAU GAELLE SUR VALIDATION==================
        nb_ll_validees = df_spe["doc_val"].notna().sum()
        pct_ll_validees = df_spe["doc_val"].notna().mean() * 100
        taux_validation_J0_over_sejours = float(
            (df_spe["sej_classe"] == "0j").mean() * 100
        )
        delai_validation_moyenne = df_spe["del_sorval"].mean()
        # ==================================================================

        stats_par_spe.append(
            {
                "specialite": str(spe),
                "total_sejours": int(total_sejours),
                "nb_sejours_valides": int(nb_ll_validees),
                "pct_sejours_validees": float(
                    pct_ll_validees
                ),  # âœ… Convertir en float natif
                "taux_validation_j0_over_sejours": float(
                    taux_validation_J0_over_sejours
                ),  # âœ… Convertir en float natif
                "delai_moyen_validation": float(delai_validation_moyenne)
                if not pd.isna(delai_validation_moyenne)
                else 0.0,  # âœ… GÃ©rer NaN
            }
        )

    # Trier par nombre total dÃ©croissant
    stats_par_spe = sorted(
        stats_par_spe, key=lambda x: x["total_sejours"], reverse=True
    )

    print(f"\n   âœ… Statistiques calculÃ©es pour {len(stats_par_spe)} spÃ©cialitÃ©s")

    return {
        "total_sejours_all": int(total_sejours_all),
        "nb_sejours_valides_all": int(nb_ll_validees_all),
        "pct_sejours_validees_all": float(
            pct_ll_validees_all
        ),  # âœ… Convertir en float natif
        "taux_validation_j0_over_sejours_all": float(
            taux_validation_J0_over_sejours_all
        ),  # âœ… Convertir en float natif
        "delai_moyen_validation_all": float(delai_validation_moyenne_all)
        if not pd.isna(delai_validation_moyenne_all)
        else 0.0,  # âœ… GÃ©rer NaN
        "par_specialite_all": stats_par_spe,
    }


def calculate_diffusion_stats(df: pd.DataFrame, matrice_path: str = None) -> Dict:
    """
    Calcule les statistiques de diffusion selon la mÃ©thodologie IQL R v7

    Indicateurs HAS:
    1. % sÃ©jours avec LL diffusÃ©e
    2. % sÃ©jours avec LL diffusÃ©e le jour de la validation

    Args:
        df: DataFrame contenant les sÃ©jours et documents
        matrice_path: Chemin vers la matrice de spÃ©cialitÃ©

    Returns:
        Dictionnaire contenant les statistiques globales et par spÃ©cialitÃ©
    """
    # Utiliser le chemin depuis settings si non fourni
    if matrice_path is None:
        matrice_path = settings.MATRICE_PATH

    print(f"\nðŸ“Š Calcul des statistiques de DIFFUSION...")

    # Classifier les sÃ©jours
    df = classify_sejours_iql(df, matrice_path)

    # Statistiques globales
    total_sejours_all = len(df)

    # =================TABLEAU GAELLE SUR DIFFUSION==================
    nb_ll_validees_all = df["doc_val"].notna().sum()

    nb_LL_diffuses_all = df["date_diffusion"].notna().sum()
    pct_diffuses_sur_validees_all = (
        nb_LL_diffuses_all / nb_ll_validees_all * 100 if nb_ll_validees_all > 0 else 0.0
    )
    pct_diffuses_sur_sejours_all = nb_LL_diffuses_all / total_sejours_all * 100

    # Convertir les dates pour calcul
    df_with_dates = df.copy()
    df_with_dates["date_diffusion"] = pd.to_datetime(df_with_dates["date_diffusion"])
    df_with_dates["doc_val"] = pd.to_datetime(df_with_dates["doc_val"])

    tx_diffusion_a_J0_validation_all = float(
        (
            (df_with_dates["date_diffusion"] - df_with_dates["doc_val"]).dt.days == 0
        ).mean()
        * 100
    )
    delai_diffusion_validation_all = (
        df_with_dates["date_diffusion"] - df_with_dates["doc_val"]
    ).dt.days.mean()

    print(f"\n   ðŸ“ˆ Statistiques globales diffusion :")
    print(
        f"      LL diffusÃ©es : {nb_LL_diffuses_all} ({pct_diffuses_sur_validees_all:.1f}% des validÃ©es)"
    )
    print(f"      DiffusÃ©es Ã  J0 validation : {tx_diffusion_a_J0_validation_all:.1f}%")

    # ==================================================================

    # Statistiques par spÃ©cialitÃ©
    stats_par_spe = []

    for spe in df["sej_spe_final"].dropna().unique():
        df_spe = df[df["sej_spe_final"] == spe]
        df_spe_dates = df_with_dates[df_with_dates["sej_spe_final"] == spe]
        total_sejours = len(df_spe)

        # =================TABLEAU GAELLE SUR DIFFUSION==================

        nb_LL_diffuses = df_spe["date_diffusion"].notna().sum()
        nb_ll_validees = df_spe["doc_val"].notna().sum()
        pct_diffuses_sur_validees = (
            nb_LL_diffuses / nb_ll_validees * 100 if nb_ll_validees > 0 else 0.0
        )
        pct_diffuses_sur_sejours = nb_LL_diffuses / total_sejours * 100

        tx_diffusion_a_J0_validation = float(
            (
                (df_spe_dates["date_diffusion"] - df_spe_dates["doc_val"]).dt.days == 0
            ).mean()
            * 100
        )
        delai_diffusion_validation = (
            df_spe_dates["date_diffusion"] - df_spe_dates["doc_val"]
        ).dt.days.mean()

        stats_par_spe.append(
            {
                "specialite": str(spe),
                "total_sejours": int(total_sejours),
                "nb_ll_diffusees": int(nb_LL_diffuses),
                "pct_ll_diffusees_over_validees": float(
                    pct_diffuses_sur_validees
                ),  # âœ… Convertir en float natif
                "pct_ll_diffusees_over_sejours": float(
                    pct_diffuses_sur_sejours
                ),  # âœ… Convertir en float natif
                "taux_diffusion_J0_validation": float(
                    tx_diffusion_a_J0_validation
                ),  # âœ… Convertir en float natif
                "delai_diffusion_validation": float(delai_diffusion_validation)
                if not pd.isna(delai_diffusion_validation)
                else 0.0,
            }
        )

    # Trier par nombre total dÃ©croissant
    stats_par_spe = sorted(
        stats_par_spe, key=lambda x: x["total_sejours"], reverse=True
    )

    print(
        f"\n   âœ… Statistiques diffusion calculÃ©es pour {len(stats_par_spe)} spÃ©cialitÃ©s"
    )

    return {
        "nb_ll_diffusees_all": int(nb_LL_diffuses_all),
        "pct_ll_diffusees_over_validees_all": float(
            pct_diffuses_sur_validees_all
        ),  # âœ… Convertir en float natif
        "pct_ll_diffusees_over_sejours_all": float(
            pct_diffuses_sur_sejours_all
        ),  # âœ… Convertir en float natif
        "taux_diffusion_J0_validation_all": float(
            tx_diffusion_a_J0_validation_all
        ),  # âœ… Convertir en float natif
        "delai_diffusion_validation_all": float(delai_diffusion_validation_all)
        if not pd.isna(delai_diffusion_validation_all)
        else 0.0,
        "par_specialite": stats_par_spe,
    }
